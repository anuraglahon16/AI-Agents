# 3.4 Testing and Debugging AI Agents

Testing and debugging AI agents presents unique challenges compared to traditional software development. The non-deterministic nature of LLM outputs, complex interactions with tools and environments, and the difficulty of defining "correct" behavior all require specialized approaches. This section explores comprehensive strategies for testing, debugging, and optimizing AI agents to ensure they perform reliably in production environments.

## Testing Strategies for AI Agents

### Overview

Effective testing of AI agents requires a multi-faceted approach that addresses their unique characteristics. Unlike traditional software with deterministic outputs, AI agents exhibit variability in responses, making testing more complex. A comprehensive testing strategy combines different testing types to ensure agent reliability across various scenarios.

### Functional Testing

1. **Capability Verification**
   - Testing core agent functionalities
   - Verifying tool usage correctness
   - Validating basic interaction patterns
   - Example: Testing that a customer service agent can correctly access product information, process returns, and escalate issues when appropriate

2. **Edge Case Identification**
   - Testing unusual or extreme inputs
   - Boundary condition exploration
   - Unexpected interaction patterns
   - Example: Testing how an agent handles incomplete information, contradictory requirements, or extremely long or complex queries

3. **Failure Mode Analysis**
   - Identifying potential failure points
   - Testing graceful degradation
   - Recovery mechanism verification
   - Example: Testing how an agent behaves when a critical tool is unavailable or when it encounters information outside its knowledge domain

4. **Implementation Approaches**
   - Test case libraries for core capabilities
   - Systematic edge case generation
   - Failure injection testing
   - Example: Creating a comprehensive test suite that covers all agent capabilities with both standard and edge cases, plus deliberate failure scenarios

### Behavioral Testing

1. **Prompt Adherence**
   - Testing alignment with system prompt
   - Verifying constraint compliance
   - Checking role consistency
   - Example: Testing whether a financial advisor agent consistently avoids giving specific investment advice as specified in its constraints

2. **Reasoning Quality**
   - Evaluating logical consistency
   - Assessing problem-solving approaches
   - Verifying factual accuracy
   - Example: Testing whether an agent's reasoning process follows logical steps and reaches sound conclusions based on available information

3. **Response Appropriateness**
   - Evaluating tone and style
   - Checking for harmful content
   - Assessing helpfulness
   - Example: Testing whether a customer service agent maintains a professional, helpful tone even when faced with ambiguous or frustrated user queries

4. **Implementation Approaches**
   - Rubric-based evaluation
   - Comparative assessment
   - Human evaluation panels
   - Example: Developing a detailed rubric for assessing agent responses across dimensions like accuracy, helpfulness, safety, and alignment with brand voice

### Performance Testing

1. **Response Time**
   - Measuring end-to-end latency
   - Identifying processing bottlenecks
   - Testing under various loads
   - Example: Measuring how quickly an agent can respond to different query types, from simple questions to complex multi-step tasks

2. **Throughput Capacity**
   - Determining maximum query handling
   - Concurrent user support
   - Resource utilization patterns
   - Example: Testing how many simultaneous users an agent system can support before response times degrade

3. **Stability Under Load**
   - Long-running reliability
   - Resource leak detection
   - Error rate under stress
   - Example: Running an agent system continuously for several days with varying load patterns to identify stability issues

4. **Implementation Approaches**
   - Automated load testing
   - Performance profiling
   - Synthetic workload generation
   - Example: Creating scripts that simulate realistic user interactions at increasing scale to identify performance limits and bottlenecks

### Security and Safety Testing

1. **Prompt Injection Resistance**
   - Testing against prompt manipulation
   - System prompt override attempts
   - Role confusion attacks
   - Example: Attempting to make an agent ignore its constraints by embedding instructions in user queries

2. **Data Leakage Prevention**
   - Testing information boundaries
   - Sensitive data handling
   - Privacy protection mechanisms
   - Example: Verifying that an agent doesn't reveal internal system details or confidential information even when directly asked

3. **Harmful Output Prevention**
   - Testing content safety filters
   - Inappropriate request handling
   - Ethical boundary enforcement
   - Example: Systematically testing how an agent responds to requests for potentially harmful information or assistance with unethical activities

4. **Implementation Approaches**
   - Red team exercises
   - Adversarial testing
   - Vulnerability scanning
   - Example: Having security specialists attempt to manipulate the agent into violating its constraints or revealing sensitive information

### Automated Testing Frameworks

1. **Test Case Generation**
   - Programmatic test creation
   - Coverage-based generation
   - Parameterized testing
   - Example: Creating a system that automatically generates test cases covering all agent capabilities and common edge cases

2. **Evaluation Automation**
   - Automated response scoring
   - Regression detection
   - Continuous testing pipelines
   - Example: Implementing automated evaluation of agent responses against predefined criteria, with alerts for any regressions

3. **Simulation Environments**
   - Synthetic user simulation
   - Scenario-based testing
   - Environment modeling
   - Example: Creating a simulated environment where virtual users interact with the agent in realistic scenarios

4. **Implementation Example**

```python
import json
import random
import time
from typing import List, Dict, Any, Callable
from langchain.agents import AgentExecutor
from langchain.evaluation import StringEvaluator

class AgentTestingFramework:
    def __init__(self, agent_executor: AgentExecutor, evaluator: StringEvaluator = None):
        """Initialize the testing framework with an agent executor and optional evaluator."""
        self.agent = agent_executor
        self.evaluator = evaluator
        self.test_cases = []
        self.results = []
    
    def add_test_case(self, test_id: str, input_query: str, expected_behavior: Dict[str, Any], 
                      category: str = "functional", tags: List[str] = None):
        """Add a test case to the framework."""
        self.test_cases.append({
            "test_id": test_id,
            "input_query": input_query,
            "expected_behavior": expected_behavior,
            "category": category,
            "tags": tags or []
        })
    
    def add_test_cases_from_file(self, file_path: str):
        """Load test cases from a JSON file."""
        with open(file_path, 'r') as f:
            test_cases = json.load(f)
        for tc in test_cases:
            self.add_test_case(**tc)
    
    def run_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Run a single test case and return the result."""
        start_time = time.time()
        
        try:
            # Execute the agent on the test input
            response = self.agent.invoke({"input": test_case["input_query"]})
            output = response.get("output", "")
            
            # Calculate response time
            response_time = time.time() - start_time
            
            # Evaluate the response if an evaluator is provided
            evaluation = None
            if self.evaluator:
                evaluation = self.evaluator.evaluate_strings(
                    prediction=output,
                    reference=test_case.get("expected_behavior", {}).get("sample_response", ""),
                    input=test_case["input_query"]
                )
            
            # Check for expected tools usage if specified
            tools_check = self._check_tools_usage(
                response.get("intermediate_steps", []),
                test_case.get("expected_behavior", {}).get("expected_tools", [])
            )
            
            # Prepare the result
            result = {
                "test_id": test_case["test_id"],
                "status": "passed" if self._is_test_passed(output, test_case, evaluation, tools_check) else "failed",
                "response": output,
                "response_time": response_time,
                "evaluation": evaluation,
                "tools_check": tools_check
            }
            
        except Exception as e:
            # Handle any exceptions during test execution
            result = {
                "test_id": test_case["test_id"],
                "status": "error",
                "error": str(e),
                "response_time": time.time() - start_time
            }
        
        return result
    
    def _check_tools_usage(self, intermediate_steps, expected_tools):
        """Check if the agent used the expected tools."""
        if not expected_tools:
            return {"status": "not_applicable"}
        
        used_tools = [step[0].tool for step in intermediate_steps]
        
        missing_tools = [tool for tool in expected_tools if tool not in used_tools]
        unexpected_tools = [tool for tool in used_tools if tool not in expected_tools]
        
        return {
            "status": "passed" if not missing_tools and not unexpected_tools else "failed",
            "used_tools": used_tools,
            "missing_tools": missing_tools,
            "unexpected_tools": unexpected_tools
        }
    
    def _is_test_passed(self, output, test_case, evaluation, tools_check):
        """Determine if a test passed based on various criteria."""
        # Check if there are specific pass criteria in the test case
        criteria = test_case.get("expected_behavior", {}).get("pass_criteria", {})
        
        # If no specific criteria, use evaluation score if available
        if not criteria and evaluation and "score" in evaluation:
            return evaluation["score"] >= 0.7  # Default threshold
        
        # Check content requirements if specified
        if "required_content" in criteria:
            for content in criteria["required_content"]:
                if content.lower() not in output.lower():
                    return False
        
        # Check forbidden content if specified
        if "forbidden_content" in criteria:
            for content in criteria["forbidden_content"]:
                if content.lower() in output.lower():
                    return False
        
        # Check tools usage if it was part of the test
        if tools_check["status"] != "not_applicable" and tools_check["status"] == "failed":
            return False
        
        # Default to passed if no criteria failed
        return True
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run all test cases and return the results."""
        self.results = []
        
        for test_case in self.test_cases:
            result = self.run_test(test_case)
            self.results.append(result)
        
        # Compile summary statistics
        total = len(self.results)
        passed = sum(1 for r in self.results if r["status"] == "passed")
        failed = sum(1 for r in self.results if r["status"] == "failed")
        errors = sum(1 for r in self.results if r["status"] == "error")
        
        avg_response_time = sum(r.get("response_time", 0) for r in self.results) / total if total > 0 else 0
        
        return {
            "total_tests": total,
            "passed": passed,
            "failed": failed,
            "errors": errors,
            "pass_rate": passed / total if total > 0 else 0,
            "avg_response_time": avg_response_time,
            "results": self.results
        }
    
    def run_tests_by_category(self, category: str) -> Dict[str, Any]:
        """Run all test cases in a specific category."""
        category_tests = [tc for tc in self.test_cases if tc["category"] == category]
        results = []
        
        for test_case in category_tests:
            result = self.run_test(test_case)
            results.append(result)
        
        # Compile summary statistics
        total = len(results)
        passed = sum(1 for r in results if r["status"] == "passed")
        
        return {
            "category": category,
            "total_tests": total,
            "passed": passed,
            "pass_rate": passed / total if total > 0 else 0,
            "results": results
        }
    
    def export_results(self, file_path: str):
        """Export test results to a JSON file."""
        with open(file_path, 'w') as f:
            json.dump({
                "timestamp": time.time(),
                "total_tests": len(self.results),
                "results": self.results
            }, f, indent=2)

# Example usage:
"""
# Create an agent (not shown here)
from langchain.agents import create_openai_functions_agent
agent_executor = create_openai_functions_agent(...)

# Initialize the testing framework
testing_framework = AgentTestingFramework(agent_executor)

# Add test cases
testing_framework.add_test_case(
    test_id="product_search_01",
    input_query="I'm looking for a laptop under $1000 for gaming",
    expected_behavior={
        "pass_criteria": {
            "required_content": ["gaming laptop", "under $1000"],
            "forbidden_content": ["over your budget"]
        },
        "expected_tools": ["product_search", "get_product_details"]
    },
    category="functional",
    tags=["product", "search", "budget"]
)

# Run tests
results = testing_framework.run_all_tests()
print(f"Pass rate: {results['pass_rate'] * 100:.2f}%")

# Export results
testing_framework.export_results("agent_test_results.json")
"""
```

### Human-in-the-Loop Evaluation

1. **Expert Review Processes**
   - Structured evaluation protocols
   - Domain expert assessment
   - Qualitative feedback collection
   - Example: Having subject matter experts review agent responses to complex queries in their domain, providing detailed feedback on accuracy and appropriateness

2. **User Acceptance Testing**
   - Real user interaction testing
   - Satisfaction measurement
   - Usability assessment
   - Example: Conducting supervised sessions where target users interact with the agent and provide feedback on their experience

3. **Comparative Evaluation**
   - A/B testing different agent versions
   - Benchmarking against baselines
   - Preference testing
   - Example: Having evaluators compare responses from different agent versions to the same queries, indicating which they prefer and why

4. **Implementation Approaches**
   - Structured evaluation forms
   - Blind comparison protocols
   - Feedback aggregation systems
   - Example: Implementing a systematic review process where evaluators rate agent responses across multiple dimensions using standardized rubrics

## Debugging Techniques

### Overview

Debugging AI agents requires specialized techniques that address their unique characteristics. The non-deterministic nature of LLM outputs, complex interactions with tools, and the difficulty of tracing reasoning processes all present challenges that traditional debugging approaches don't fully address.

### Observability Implementation

1. **Comprehensive Logging**
   - Capturing all agent inputs and outputs
   - Recording intermediate reasoning steps
   - Tracking tool usage and results
   - Example: Implementing structured logging that captures the com
(Content truncated due to size limit. Use line ranges to read in chunks)